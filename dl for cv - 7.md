
---

## Lecture 7: Convolutional Networks â€” Detailed Skeleton

---

### 0. **Lecture Metadata**

- **Instructor:** Justin Johnson, University of Michigan, 2019 Deep Learning for Computer Vision
    
- **Topics Covered:** Convolution, Pooling, Batch Normalization
    
- **Video Duration:** ~1:12:03 (YouTube) ([web.eecs.umich.edu](https://web.eecs.umich.edu/~justincj/teaching/eecs498/FA2019/schedule.html?utm_source=chatgpt.com "Schedule | EECS 498-007 / 598-005: Deep Learning for ..."))
    
- **Korean note summary**: available for conceptual guidance ([The man in the Arena](https://life-ai-learning.tistory.com/entry/EECS-498-007-598-005-Lecture-7-Convolutional-Networks?category=1148106 "[EECS 498-007 / 598-005] Lecture 7: Convolutional	Networks"))
    

---

### 1. **Motivation & Biological Inspiration** (0:00 â€“ ~5:00)

- **Problem with MLPs on images:**
    
    - Flattening 2D spatial data loses structure.
        
    - Parameter explosion: e.g., 400Ã—700Ã—3 image â†’ 840,000 inputs, millions of weights even for a small hidden layer. ([The man in the Arena](https://life-ai-learning.tistory.com/entry/EECS-498-007-598-005-Lecture-7-Convolutional-Networks?category=1148106 "[EECS 498-007 / 598-005] Lecture 7: Convolutional	Networks"))
        
- **Biological analogy (Hubel & Wiesel):**
    
    - Neurons respond to local regions (receptive fields) in the visual cortex. Circa 1960sâ€”foundation for convolution. ([cs231n.stanford.edu](https://cs231n.stanford.edu/slides/2016/winter1516_lecture7.pdf?utm_source=chatgpt.com "Lecture 7: Convolutional Neural Networks"))
        

---

### 2. **Convolutional Layer Fundamentals** (~5:00 â€“ ~20:00)

- **Definition**:
    
    - Filter = small spatial window (e.g., 5Ã—5), applied across image via dot product.
        
        - Takes full depth of input.
            
    - Results in a feature map (activation map). ([cs231n.stanford.edu](https://cs231n.stanford.edu/slides/2016/winter1516_lecture7.pdf?utm_source=chatgpt.com "Lecture 7: Convolutional Neural Networks"))
        
- **Stacking multiple filters** â†’ multiple activation maps â†’ form new depth channels.
    
- **Parameter computation example**:
    
    - Input: 32Ã—32Ã—3, filter: 5Ã—5Ã—3, stride 1 â†’ output: 28Ã—28 spatially.
        
    - If 10 filters: output = 28Ã—28Ã—10. ([cs231n.stanford.edu](https://cs231n.stanford.edu/slides/2016/winter1516_lecture7.pdf?utm_source=chatgpt.com "Lecture 7: Convolutional Neural Networks"))
        
    - Number of parameters = (filter_size Ã— depth + bias) Ã— number_filters. Example: 76 Ã— 10 = 760. ([cs231n.stanford.edu](https://cs231n.stanford.edu/slides/2016/winter1516_lecture7.pdf?utm_source=chatgpt.com "Lecture 7: Convolutional Neural Networks"))
        
- **Hyperparameters**:
    
    - Stride, padding, filter size control resolution and spatial shrinkage. Using zero-padding of (Fâˆ’1)/2 preserves spatial size. ([cs231n.stanford.edu](https://cs231n.stanford.edu/slides/2016/winter1516_lecture7.pdf?utm_source=chatgpt.com "Lecture 7: Convolutional Neural Networks"))
        

---

### 3. **Activation & Pooling Layers** (~20:00 â€“ ~30:00)

- **ReLU (nonlinearity)**:
    
    - Applied elementwise after convolution, keeps positive values, introduces nonlinearity.
        
- **Pooling (e.g., 2Ã—2 max pooling)**:
    
    - Downsamples spatial dimensions, retains depth.
        
    - Introduces modest translation invariance. ([cs231n.stanford.edu](https://cs231n.stanford.edu/slides/2016/winter1516_lecture7.pdf?utm_source=chatgpt.com "Lecture 7: Convolutional Neural Networks"))
        
- **Layer Pattern Summary**:
    
    - Build CNNs via repeating **[Conv â†’ ReLU â†’ Pool]** blocks.
        

---

### 4. **CNN as a Hierarchical Feature Extractor** (~30:00 â€“ ~40:00)

- Early filters detect edges/colors.
    
- Subsequent layers detect textures, parts, and eventually semantic concepts.
    
- CNN layers form a **hierarchy of feature abstraction**â€”all feed into deeper layers (shown via visuals) ([cs231n.stanford.edu](https://cs231n.stanford.edu/slides/2016/winter1516_lecture7.pdf?utm_source=chatgpt.com "Lecture 7: Convolutional Neural Networks"))
    

---

### 5. **CNN Forward & Backward Mechanics** (~40:00 â€“ ~50:00)

- **Forward pass**: Convolution â†’ ReLU â†’ Pool â†’ (repeat).
    
- **Backward pass**:
    
    - Shared filter weights receive gradients from multiple positions.
        
    - Pooling layers backward-pass only to maxima locations.
        
    - Backprop remains chain-rule â€” Conv differs only in weight sharing/local connectivity.
        

---

### 6. **Flatten + Fully Connected + Softmax** (~50:00 â€“ ~55:00)

- After several conv/pool stages, flatten the activation volumes.
    
- Feed into FC layers for classification.
    
- Same softmax-cross-entropy loss as in Lecture 4.
    

---

### 7. **Batch Normalization (BN)** (~55:00 â€“ ~65:00)

- **Purpose**: Mitigates internal covariate shiftâ€”normalizes activations within mini-batches.
    
- **Process**:
    
    - Compute batch mean and variance, normalize activations.
        
    - Scale and offset via learned parameters (Î³, Î²).
        
    - In testing: use running averages, not batch stats.
        
- BN **stabilizes training**, allows for higher learning rates, acts like a regularizer.
    

---

### 8. **Summary Slide & In-class FAQ** (~65:00 â€“ ~72:00)

- **Typical CNN architecture formula**:
    
    ```
    [(Conv â†’ ReLU) * N â†’ Pool?] * M â†’ (FC â†’ ReLU) * K â†’ Softmax
    ```
    
- Jonathan may illustrate via CIFAR-10 demo (ConvNetJS) or show practical filters learned visually.
    
- He might also link to Assignment 2 where CNN modules build upon what you already coded as FC.
    

---

## Summary Outline (For Your Notes)

```
Lecture 7 â€“ Convolutional Neural Networks
A. Motivation (MLPs inefficient for images; biological receptive fields)
B. Convolutional Layer
   1. Filter sliding = local connectivity
   2. Multiple filters = activation map depth
   3. Parameter count & example math
   4. Hyperparameters (stride, padding, size)
C. Activation & Pooling
   5. ReLU nonlinearity
   6. Max pooling and translation invariance
   7. ConvNet block pattern
D. Hierarchical Feature Extraction
E. Forward/Backward Mechanics (backprop with conv/pool)
F. Flatten â†’ FC â†’ Softmax (classification layer)
G. Batch Normalization: training vs test behavior, benefits
H. Recap Slide & Demo Examples
```

---

###
![[Pasted image Ù¢Ù Ù¢Ù¥Ù Ù¨Ù¡Ù§Ù¢Ù¢Ù¥Ù¤Ù¡Ù¦.png]]
#### ğŸ”¹ 1. Input + Convolution

- The **image (input)** is passed through a **kernel (filter of weights)**.
    
- Each filter learns to detect a certain feature (edges, curves, textures, colors, etc.).
    
- The result is a **feature map** (also called activation map).
    

ğŸ‘‰ **Key:** Multiple filters = multiple feature maps. They donâ€™t collapse â€” they coexist as separate channels.

---

#### ğŸ”¹ 2. Non-linearity (ReLU)

- After convolution, you apply a **ReLU (Rectified Linear Unit)** to each feature map **independently**.
    
- ReLU sets all negative activations to 0 but keeps positive values.
    
- This introduces **non-linearity**, which prevents the CNN from being just a linear transformation (which would collapse feature maps into something like one big linear combination).
    

ğŸ‘‰ **So:** ReLU doesnâ€™t â€œconnectâ€ different feature maps. It just cleans and sharpens each one separately.

---

#### ğŸ”¹ 3. Pooling

- Pooling (often max pooling) reduces the **spatial dimension** (height Ã— width), but **not the depth** (number of feature maps).
    
- If you had 32 feature maps before pooling, youâ€™ll still have 32 after pooling â€” just smaller (compressed) ones.
    

ğŸ‘‰ Pooling makes the representation more compact and translation-invariant.

---

#### ğŸ”¹ 4. Stacking Layers

- You then apply another convolution layer â†’ more filters â†’ **new set of feature maps**.
    
- This process repeats: Convolution â†’ ReLU â†’ Pooling â†’ â€¦
    
- Each deeper layer combines lower-level features (edges) into higher-level features (shapes, eyes, wheels, etc.).
    

ğŸ‘‰ So the feature maps stack like hidden layers in a regular NN, but they preserve spatial structure.

---

#### ğŸ”¹ 5. Flatten â†’ Fully Connected Layers

- After several convolution + pooling stages, you **flatten** the 3D tensor (height Ã— width Ã— depth of feature maps) into a long vector.
    
- This vector goes into a **fully connected layer** (like a normal NN).
    
- The last fully connected layer feeds into a **softmax**, which outputs probabilities for each class.
    

---

#### ğŸ”¹ To Answer Your Question Directly:

- **Do activation maps collapse into one?** â†’ No. Each filter produces its own activation map, and they stack as channels.
    
- **What connects them?** â†’ Nothing mixes them at the convolution stage; they are parallel. Mixing happens later when multiple filters feed into the next convolution (each new filter takes inputs across _all previous feature maps_).
    
- **What does ReLU do?** â†’ Applied independently per feature map, introduces nonlinearity, keeps positive signals only.
    
- **Is there anything else besides ReLU?** â†’ Yes, sometimes other nonlinearities are used (Sigmoid, Tanh, Leaky ReLU, ELU, GELUâ€¦), but ReLU is standard because itâ€™s simple and avoids vanishing gradients.
    

---

ğŸ‘‰ So the diagram shows **Representation (left: conv+ReLU+pool)** and **Classification (right: flatten + FC + softmax)**.  
The feature maps donâ€™t collapse into one â€” they grow into a richer, deeper set until flattening.

---
- MLPs require flattening images, **destroying spatial structure**â€”like treating every pixel as unrelated. This makes learning both inefficient and prone to overfitting + to actually connect them in fully connected nn => no. of parameters will increase exponentially.
    
![[Recording Ù¢Ù Ù¢Ù¥Ù Ù¨Ù¡Ù§Ù¡Ù¦Ù¤Ù¥Ù¤Ù .webm]]

) L2â€“L6 (what you already built)

- You learned the **generic pipeline**: input â†’ features â†’ classifier â†’ loss â†’ backprop/optim.
    
- In practice you used **fully connected (FC) layers** (your hand-drawn Wâ‚/Wâ‚‚ in A2).
    
- The slide you translated shows **why FC on images explodes parameter count** (e.g., 400Ã—700Ã—3 inputs â†’ 840k inputs â†’ billions of weights if you use a modest hidden layer).
    
    - Consequences: slow learning, overfitting risk, and you **ignore spatial structure** when you flatten.
        

### 2) L7â€“L9 (what youâ€™re starting now)

- CNNs are introduced **exactly to fix those issues**:
    
    - **Local connectivity** (small receptive fields) and **weight sharing** (same filter slides) â†’ **huge parameter drop**.
        
    - **Translation tolerance** via weight sharing + (often) pooling â†’ features detected anywhere.
        
    - Same loss/optim/backprop as L2â€“L6, but with **new layer types** (conv/pool) and **structured inductive bias**.
        

> Mental model:- 
> FC on images = parameter explosion + loses geometry.
> CNN = **keeps geometry**, **slashes parameters**, **generalizes better**.

- ReLU activations and backprop remain consistent with earlier lectures, but **this structural constraint is what makes CNNs efficient and powerful**, especially visible around the start of Lecture 7 (~0:00â€“8:30)

---

### ğŸ”¹ Step 1: Filter â†’ Activation Map (Linear Step)

- Each filter does a **linear operation** (dot product).
    
- Without anything else, this is just like a big linear layer:
    
    - multiple filters would just be different linear projections of the same image.
        
    - If you stacked only linear filters, the entire CNN would still be **linear overall**.
        

That means: **stacking convolutions alone â‰ˆ still a linear model** â†’ no extra expressive power, no nonlinearity.

---

### ğŸ”¹ Step 2: Nonlinear Function (ReLU)

- After each convolution, we pass the activation map through a **nonlinear activation function** (commonly ReLU).
    
- **ReLU(x) = max(0, x)**.
    

ğŸ‘‰ What this does:

1. **Separates features**: keeps positive activations (strong matches with the filter), kills negative ones.
    
2. **Prevents collapse**: if everything stayed linear, all filters could be combined into one equivalent linear transformation. ReLU makes the system **piecewise linear** and therefore _non-collapsible_.
    
3. **Interaction between feature maps**: indirectly, yes.
    
    - Each feature map is individually ReLUâ€™d.
        
    - But because ReLU is nonlinear, when the next layer takes _all_ feature maps as input, the resulting filters can combine them in **nonlinear ways** (edges + curves = corners, etc).
        

---

### ğŸ”¹ Step 3: Stacking Multiple Feature Maps

- Each filter gives its own activation map.
    
- After ReLU, you stack them along the depth axis.
    
- Next layerâ€™s filters slide not just over the image pixels, but over **all the activation maps at once**.
    
    - So a filter in the 2nd layer is 3D: it spans (height Ã— width Ã— depth of input feature maps).
        
    - Thatâ€™s where maps â€œinteractâ€ â€” not directly with each other, but through the **next set of filters**.
        

---

### ğŸ”¹ Step 4: Beyond ReLU (Other Nonlinearities)

- CNNs can use other nonlinearities, though ReLU is most common:
    
    - **Sigmoid / Tanh** (older, but vanish gradients).
        
    - **Leaky ReLU / ELU** (variants that let negative info leak through).
        
    - **Maxout** (rare, but learns its own activation).
        

But Jonathan (2019) focuses on **ReLU**, because itâ€™s simple and works really well.

---

### ğŸŒ± To tie this back to your question:

- Filters by themselves would indeed just give different _linear_ maps.
    
- ReLU makes them **nonlinear + separable**.
    
- Next-layer filters mix them together again â†’ giving rise to richer representations.
    

Thatâ€™s why CNNs work:  
**Linear (conv) â†’ Nonlinear (ReLU) â†’ Linear (conv) â†’ Nonlinear (ReLU)â€¦**  
= hierarchical feature learning.

---
