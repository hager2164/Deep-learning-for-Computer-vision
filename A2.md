# 1. SVM
- claude chat [^1]
## a. naive
### 1. What SVM loss is saying

For each training example xix_i:

- The **correct class score** should be higher than every other class score by at least a margin (say, 1).
    
- If a â€œwrongâ€ class jj has a score too close (or higher), we pay a **penalty**:
    
    marginij=sjâˆ’sy[i]+1\text{margin}_{ij} = s_j - s_{y[i]} + 1
- If margin > 0, we add it to the loss **and adjust gradients**.
    

---

### 2. Why do we touch the gradient when margin > 0?

Think:

- If class jj is stealing credit, we want to **push its weights down** (so it scores lower next time).
    
- And we want to **push the correct classâ€™s weights up** (so it scores higher next time).
    

This is why:

- dW[:,j]dW[:, j] gets **+x_i** (increase its gradient â†’ when SGD subtracts gradient, its score will go down).
    
- dW[:,y[i]]dW[:, y[i]] gets **âˆ’x_i** (decrease its gradient â†’ when SGD subtracts gradient, its score will go up).
    

So each violation (margin > 0) triggers **two updates**: one for the wrong class, one for the correct class.

---

### 3. Averaging

At the end, we divide `loss` by `num_train`.  
The gradient `dW` also needs to be divided by `num_train` (so every example contributes equally).  
ðŸ‘‰ Your code is missing this normalization step for `dW`.

---

### 4. Regularization gradient

You already added reg to the **loss** with

```python
loss += reg * torch.sum(W * W)
```

Derivative of regâ‹…âˆ¥Wâˆ¥2reg \cdot \|W\|^2 wrt WW is:

dWreg=2â‹…regâ‹…WdW_\text{reg} = 2 \cdot reg \cdot W

ðŸ‘‰ But in your code you wrote:

```python
dW += reg
```

Thatâ€™s not right â€” you should be adding something proportional to **W itself**.

---

### 5. In Pseudo-code:

For each training example ii:  
Â Â Compute scores = WTxiW^T x_i  
Â Â correct_score = score[y[i]]

Â Â For each class jâ‰ y[i]j \neq y[i]:  
Â Â Â Â if (score[j] - correct_score + 1 > 0):  
Â Â Â Â Â Â loss += margin  
Â Â Â Â Â Â dW[:, j] += x_i  
Â Â Â Â Â Â dW[:, y[i]] -= x_i

After loops:

- loss /= num_train
    
- dW /= num_train
    
- loss += reg * sum(W*W)
    
- dW += 2 * reg * W
    

---

âœ… So your **main fixes** are:

1. Normalize `dW` by `num_train`.
    
2. Regularization gradient should be `2 * reg * W`, not just `reg`.
    


## b. vectorized
```
def svm_loss_vectorized(
    W: torch.Tensor, X: torch.Tensor, y: torch.Tensor, reg: float):
    """
    Structured SVM loss function, vectorized implementation.
    
    Key insight: Instead of loops, compute ALL samples and classes at once!
    """
    loss = 0.0
    dW = torch.zeros_like(W)  # initialize the gradient as zero
    N = X.shape[0]  # number of samples
    
    # STEP 1: Compute scores for ALL samples at once
    # X: (N, D), W: (D, C) -> scores: (N, C)
    # Each scores[i,j] = score of sample i for class j
    scores = X @ W
    
    # STEP 2: Extract correct class scores for each sample
    # y contains correct class indices for each sample
    # This gives us a vector of shape (N,) with correct scores
    correct_scores = scores[range(N), y]
    
    # STEP 3: Compute margins using broadcasting
    # We want: margin[i,j] = scores[i,j] - correct_scores[i] + 1
    # correct_scores[:, None] reshapes from (N,) to (N,1) for broadcasting
    # PyTorch automatically duplicates each correct score across all columns
    margins = scores - correct_scores[:, None] + 1
    
    # STEP 4: Zero out correct class positions
    # We don't want to penalize correct class vs itself (always gives margin=1)
    # Only penalize when wrong classes have higher scores than correct class
    margins[range(N), y] = 0
    
    # STEP 5: Apply ReLU (keep only positive margins)
    # Negative margin = wrong class score < correct score = GOOD (no penalty)
    # Positive margin = wrong class score >= correct score = BAD (penalty!)
    margins = torch.clamp(margins, min=0)  # equivalent to max(margins, 0)
    
    # STEP 6: Compute loss
    # Sum all positive margins, average over samples, add regularization
    loss = torch.sum(margins) / N + reg * torch.sum(W * W)
    
    # STEP 7: Compute gradient (reuse intermediate values from loss computation)
    # Create mask: 1 where margin > 0, 0 elsewhere
    mask = (margins > 0).float()  # shape (N, C)
    
    # For each sample, count how many wrong classes contributed to loss
    mask[range(N), y] = -torch.sum(mask, dim=1)  # correct class gets negative sum
    
    # Gradient: X^T @ mask, averaged and regularized
    dW = X.T @ mask / N + 2 * reg * W
    
    return loss, dW
```

## c. simple batch -> SGD
**The goal:** Instead of using all training data at once, we want to randomly pick a smaller batch for each training step (this is called Stochastic Gradient Descent).
[^1]: https://claude.ai/share/ce069cbf-eb05-4ff3-8ffd-90f36a15bc28

# 2. softmax
## 1. naive
- 
![[Recording Ù¢Ù Ù¢Ù¥Ù Ù¨Ù¢Ù£Ù¢Ù¢Ù¡Ù§Ù¤Ù¥.webm]]

# 3. two layer network
1. forward pass:
```
hidden = X @ W1 + b1
hidden = torch.max(hidden, torch.zeros_like(hidden))  # or torch.clamp(hidden, min=0)
scores = hidden @ W2 + b2
   ```
2. backprop

	**For any weight matrix, the gradient has this form:**
	
	```python
	grads['W'] = forward_gradient + regularization_gradient
	```
	
	**The pattern is:**
	
	- `grads['W2'] = hidden.T @ dscores + 2 * reg * W2`
	- `grads['b2'] = dscores.sum(axis=0)`
	- `grads['W1'] = X.T @ dhidden_after_relu + 2 * reg * W1`
	- `grads['b1'] = dhidden_after_relu.sum(axis=0)`
	
	Where:
	
	- `dscores = probs; dscores[range(N), y] -= 1; dscores /= N`
	- `dhidden_after_relu` = gradient flowing back through ReLU

3. train
	```python
	def nn_train(
	    params: Dict[str, torch.Tensor],
	    loss_func: Callable,
	    pred_func: Callable,
	    X: torch.Tensor,
	    y: torch.Tensor,
	    X_val: torch.Tensor,
	    y_val: torch.Tensor,
	    learning_rate: float = 1e-3,
	    learning_rate_decay: float = 0.95,
	    reg: float = 5e-6,
	    num_iters: int = 100,
	    batch_size: int = 200,
	    verbose: bool = False,
	):
	    """
	    Train this neural network using stochastic gradient descent.
	    """
	    num_train = X.shape[0]
	    iterations_per_epoch = max(num_train // batch_size, 1)
	
	    # Use SGD to optimize the parameters in self.model
	    loss_history = []
	    train_acc_history = []
	    val_acc_history = []
	
	    for it in range(num_iters):
	        X_batch, y_batch = sample_batch(X, y, num_train, batch_size)
	
	        # Compute loss and gradients using the current minibatch
	        loss, grads = loss_func(params, X_batch, y=y_batch, reg=reg)
	        loss_history.append(loss.item())
	
	        # SGD PARAMETER UPDATES (key concept you learned):
	        # Move in opposite direction of gradient (gradient points toward increasing loss)
	        # Learning rate controls step size
	        params['W1'] -= learning_rate * grads['W1']
	        params['W2'] -= learning_rate * grads['W2'] 
	        params['b1'] -= learning_rate * grads['b1']
	        params['b2'] -= learning_rate * grads['b2']
	
	        if verbose and it % 100 == 0:
	            print("iteration %d / %d: loss %f" % (it, num_iters, loss.item()))
	
	        # Every epoch, check train and val accuracy and decay learning rate.
	        if it % iterations_per_epoch == 0:
	            # Check accuracy
	            y_train_pred = pred_func(params, loss_func, X_batch)
	            train_acc = (y_train_pred == y_batch).float().mean().item()
	            y_val_pred = pred_func(params, loss_func, X_val)
	            val_acc = (y_val_pred == y_val).float().mean().item()
	            train_acc_history.append(train_acc)
	            val_acc_history.append(val_acc)
	
	            # Decay learning rate
	            learning_rate *= learning_rate_decay
	
	    return {
	        "loss_history": loss_history,
	        "train_acc_history": train_acc_history,
	        "val_acc_history": val_acc_history,
	    }
	```
	
	**Key concepts you learned across all functions:**
	
	- **Forward pass:** Matrix multiplication + bias + activation
	- **Softmax loss:** Numeric stability, probabilities, negative log likelihood
	- **L2 regularization:** Penalize large weights with squared terms
	- **Backpropagation:** Gradients flow backward, chain rule
	- **SGD updates:** Move opposite to gradient direction
	
4. 